{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \"\"\"\n",
    "    Module is a super class. It could be a single layer, or a multilayer perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        h = f(z); z is the input, and h is the output.\n",
    "        \n",
    "        Inputs:\n",
    "        _input: z\n",
    "        \n",
    "        Returns:\n",
    "        output h\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        Compute:\n",
    "        gradient w.r.t. _input\n",
    "        gradient w.r.t. trainable parameters\n",
    "        \n",
    "        Inputs:\n",
    "        _input: z\n",
    "        _gradOutput: dL/dh\n",
    "        \n",
    "        Returns:\n",
    "        gradInput: dL/dz\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return the value of trainable parameters and its corresponding gradient (Used for grandient descent)\n",
    "        \n",
    "        Returns:\n",
    "        params, gradParams\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn the module into training mode. (Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn the module into evaluate mode. (Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        self._inputs = [_input]\n",
    "        for layer in self.layers: self._inputs.append(layer.forward(self._inputs[-1]))\n",
    "        return self._inputs[-1]\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        if self._inputs is None:\n",
    "            print('had to do this')\n",
    "            self.forward(_input)\n",
    "        \n",
    "        self._gradInputs = [None for _ in range(self.size() + 1)]\n",
    "        self._gradInputs[self.size()] = _gradOutput\n",
    "\n",
    "        for i in range(self.size()-1, -1, -1):\n",
    "            self._gradInputs[i] = self.layers[i].backward(self._inputs[i], self._gradInputs[i+1])\n",
    "        \n",
    "        return self._gradInputs[0]\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        gradParams = []\n",
    "        for m in self.layers:\n",
    "            _p, _g = m.parameters()\n",
    "            if _p is not None:\n",
    "                params.append(_p)\n",
    "                gradParams.append(_g)\n",
    "        return params, gradParams\n",
    "\n",
    "    def training(self):\n",
    "        Module.training(self)\n",
    "        for m in self.layers:\n",
    "            m.training()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        Module.evaluate(self)\n",
    "        for m in self.layers:\n",
    "            m.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnected(Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        Module.__init__(self)\n",
    "        stdv = 1. / np.sqrt(inputSize)\n",
    "        self.weight = np.random.uniform(-stdv, stdv, (inputSize, outputSize))\n",
    "        self.gradWeight = np.ndarray((inputSize, outputSize))\n",
    "        self.bias = np.random.uniform(-stdv, stdv, outputSize)\n",
    "        self.gradBias = np.ndarray(outputSize)\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        _input: N x inputSize\n",
    "        \"\"\"\n",
    "        return  _input @ self.weight + self.bias\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        _input: N x inputSize\n",
    "        _gradOutput: N x outputSize\n",
    "        \"\"\"\n",
    "        self.gradWeight = _input.T @ _gradOutput\n",
    "        self.gradBias = np.sum(_gradOutput, axis=0)\n",
    "        return _gradOutput @ self.weight.T\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias], [self.gradWeight, self.gradBias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    ReLU activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        _input: N x d\n",
    "        \"\"\"\n",
    "        return _input * (_input > 0)\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        _input: N x d\n",
    "        _gradOutput: N x d\n",
    "        \"\"\"\n",
    "        return _gradOutput * (_input > 0)\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parameters, return None\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "class Logistic(Module):\n",
    "    \"\"\"\n",
    "    Logistic activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        pass\n",
    "        \n",
    "    def parameters(self):\n",
    "        return None, None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "class Dropout(Module):\n",
    "    \"\"\"\n",
    "    A dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p = 0.5):\n",
    "        Module.__init__(self)\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        pass\n",
    "    \n",
    "    def parameters(self):\n",
    "        return None, None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "class SoftMaxLoss(object):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def logits(_input):\n",
    "        return _input - logsumexp(_input, axis=1)[:, np.newaxis]\n",
    "        \n",
    "    def forward(self, _input, _label):\n",
    "        \"\"\"\n",
    "        _input: N x C\n",
    "        _labels: N x C, one-hot\n",
    "        \"\"\"\n",
    "        xent = np.sum(self.logits(_input) * _label)\n",
    "        return -xent / _input.shape[0]\n",
    "    \n",
    "    def backward(self, _input, _label):\n",
    "        probs = np.exp(self.logits(_input))\n",
    "        return (probs - _label) / _input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02254328 -0.31089954  0.02081726  0.02107968  0.04382796  0.02676907\n",
      "   0.04459196  0.04099092  0.04747656  0.04280285]\n",
      " [ 0.05323232  0.03528053 -0.28669873  0.02875008  0.02587762  0.0329954\n",
      "   0.02502313  0.0352213   0.02558782  0.02473053]\n",
      " [ 0.02522514  0.03811736  0.03078141 -0.29795639  0.03458867  0.04492276\n",
      "   0.02989044  0.03091896  0.04091967  0.02259198]] \n",
      "\n",
      "[[ 0.02254328 -0.31089954  0.02081726  0.02107968  0.04382796  0.02676907\n",
      "   0.04459196  0.04099092  0.04747656  0.04280285]\n",
      " [ 0.05323232  0.03528053 -0.28669873  0.02875008  0.02587762  0.0329954\n",
      "   0.02502313  0.0352213   0.02558782  0.02473053]\n",
      " [ 0.02522514  0.03811736  0.03078141 -0.29795639  0.03458867  0.04492276\n",
      "   0.02989044  0.03091896  0.04091967  0.02259198]] \n",
      "\n",
      "4.204230978652034e-09\n"
     ]
    }
   ],
   "source": [
    "def test_sm():\n",
    "    crit = SoftMaxLoss()\n",
    "    gt = np.zeros((3, 10))\n",
    "    gt[np.arange(3), np.array([1,2,3])] = 1\n",
    "    x = np.random.random((3,10))\n",
    "    \n",
    "    gradInput = crit.backward(x, gt)\n",
    "    gradInput_num = numeric_gradient(lambda x: crit.forward(x, gt), x, 1, 1e-6)\n",
    "    \n",
    "    print(gradInput, '\\n')\n",
    "    print(gradInput_num, '\\n')\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "    \n",
    "test_sm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.FullyConnected'>\n",
      "8.62352845479951e-09\n",
      "<class '__main__.ReLU'>\n",
      "5.962448038856628e-10\n",
      "<class '__main__.Sequential'>\n",
      "1.1374970739389082e-09\n"
     ]
    }
   ],
   "source": [
    "def test_module(model):\n",
    "    model.evaluate()\n",
    "\n",
    "    crit = TestCriterion()\n",
    "    gt = np.random.random((3,10))\n",
    "    x = np.random.random((3,10))\n",
    "\n",
    "    gradInput = model.backward(x, crit.backward(model.forward(x), gt))\n",
    "    gradInput_num = numeric_gradient(lambda x: crit.forward(model.forward(x), gt), x, 1, 1e-6)\n",
    "    print(type(model))\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "\n",
    "# Test fully connected\n",
    "model = FullyConnected(10, 10)\n",
    "test_module(model)\n",
    "\n",
    "# Test ReLU\n",
    "model = ReLU()\n",
    "test_module(model)\n",
    "\n",
    "# Test Sequential\n",
    "model = Sequential()\n",
    "model.add(FullyConnected(10, 10))\n",
    "model.add(ReLU())\n",
    "test_module(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10793826852209513\n",
      "0.06193538576992093\n",
      "0.0368724015215882\n",
      "0.01915179448301896\n",
      "0.01117941054495426\n",
      "0.007954375851647552\n",
      "0.00854380972191881\n",
      "0.0068864435765764245\n",
      "0.008792284582334785\n",
      "0.0087563483625781\n",
      "0.009099926045734778\n"
     ]
    }
   ],
   "source": [
    "# Test gradient descent, the loss should be lower and lower\n",
    "trainX = np.random.random((10,5))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(FullyConnected(5, 3))\n",
    "model.add(ReLU())\n",
    "model.add(FullyConnected(3, 1))\n",
    "\n",
    "crit = TestCriterion()\n",
    "\n",
    "it = 0\n",
    "state = None\n",
    "while it <= 1000:\n",
    "    output = model.forward(trainX)\n",
    "    loss = crit.forward(output, None)\n",
    "    if it % 100 == 0: print(loss)\n",
    "    \n",
    "    doutput = crit.backward(output, None)\n",
    "    model.backward(trainX, doutput)\n",
    "    \n",
    "    params, gradParams = model.parameters()\n",
    "    sgd(params, gradParams, 0.01, 0.8)\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start to work on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load large trainset.\n",
      "(7000, 576)\n",
      "(7000, 10)\n",
      "Load valset.\n",
      "(2000, 576)\n",
      "(2000, 10)\n"
     ]
    }
   ],
   "source": [
    "import MNIST_utils\n",
    "data_fn = \"CLEAN_MNIST_SUBSETS.h5\"\n",
    "\n",
    "# We only consider large set this time\n",
    "print(\"Load large trainset.\")\n",
    "Xlarge, Ylarge = MNIST_utils.load_data(data_fn, \"large_train\")\n",
    "print(Xlarge.shape)\n",
    "print(Ylarge.shape)\n",
    "\n",
    "print(\"Load valset.\")\n",
    "Xval, Yval = MNIST_utils.load_data(data_fn, \"val\")\n",
    "print(Xval.shape)\n",
    "print(Yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "    \"\"\"\n",
    "    Evaluate the soft predictions of the model.\n",
    "    Input:\n",
    "    X : N x d array (no unit terms)\n",
    "    model : a multi-layer perceptron\n",
    "    Output:\n",
    "    yhat : N x C array\n",
    "        yhat[n][:] contains the score over C classes for X[n][:]\n",
    "    \"\"\"\n",
    "    return model.forward(X)\n",
    "\n",
    "def error_rate(X, Y, model):\n",
    "    \"\"\"\n",
    "    Compute error rate (between 0 and 1) for the model\n",
    "    \"\"\"\n",
    "    model.evaluate()\n",
    "    res = 1 - (model.forward(X).argmax(-1) == Y.argmax(-1)).mean()\n",
    "    model.training()\n",
    "    return res\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def runTrainVal(X, Y, model, Xval, Yval, trainopt):\n",
    "    \"\"\"\n",
    "    Run the train + evaluation on a given train/val partition\n",
    "    trainopt: various (hyper)parameters of the training procedure\n",
    "    During training, choose the model with the lowest validation error. (early stopping)\n",
    "    \"\"\"\n",
    "    \n",
    "    eta = trainopt['eta']\n",
    "    \n",
    "    N = X.shape[0] # number of data points in X\n",
    "    \n",
    "    # Save the model with lowest validation error\n",
    "    minValError = np.inf\n",
    "    saved_model = None\n",
    "    \n",
    "    shuffled_idx = np.random.permutation(N)\n",
    "    start_idx = 0\n",
    "    last_val = None\n",
    "    for iteration in range(trainopt['maxiter']):\n",
    "        if iteration % int(trainopt['eta_frac'] * trainopt['maxiter']) == 0:\n",
    "            eta *= trainopt['etadrop']\n",
    "            \n",
    "        # form the next mini-batch\n",
    "        stop_idx = min(start_idx + trainopt['batch_size'], N)\n",
    "        batch_idx = range(N)[int(start_idx):int(stop_idx)]\n",
    "        bX = X[shuffled_idx[batch_idx],:]\n",
    "        bY = Y[shuffled_idx[batch_idx],:]\n",
    "\n",
    "        score = model.forward(bX)\n",
    "        loss = crit.forward(score, bY)\n",
    "        dscore = crit.backward(score, bY)\n",
    "        model.backward(bX, dscore)\n",
    "        \n",
    "        # update\n",
    "        params, gradParams = model.parameters()\n",
    "        sgd(params, gradParams, eta, weight_decay = trainopt['lambda'])    \n",
    "        start_idx = stop_idx % N\n",
    "        \n",
    "        if (iteration % trainopt['display_iter']) == 0:\n",
    "            # compute train and val error; multiply by 100 for readability (make it percentage points)\n",
    "            trainError = 100 * error_rate(X, Y, model)\n",
    "            valError = 100 * error_rate(Xval, Yval, model)\n",
    "            print('{:8} batch loss: {:.3f} train error: {:.3f} val error: {:.3f}'.format(iteration, loss, trainError, valError))\n",
    "            \n",
    "            # early stopping criterion\n",
    "            if last_val is not None and valError > last_val * (1 - 1e-3): break\n",
    "            last_val = valError\n",
    "            \n",
    "            if valError < minValError:\n",
    "                saved_model = deepcopy(model)\n",
    "                minValError = valError\n",
    "        \n",
    "    return saved_model, minValError, trainError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activation = {'ReLU': ReLU}\n",
    "\n",
    "def build_model(input_size, hidden_size, output_size, activation_func='ReLU', dropout=0):\n",
    "    \"\"\"\n",
    "    Build the model:\n",
    "    input_size: the dimension of input data\n",
    "    hidden_size: the dimension of hidden vector\n",
    "    output_size: the output size of final layer.\n",
    "    activation_func: ReLU, Logistic, Tanh, etc. (Need to be implemented by yourself)\n",
    "    dropout: the dropout rate: if dropout == 0, this is equivalent to no dropout\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(FullyConnected(input_size, hidden_size))\n",
    "    model.add(activation[activation_func]())\n",
    "    model.add(FullyConnected(hidden_size, output_size))\n",
    "    return model\n",
    "\n",
    "def build_model_custom(input_size, *hidden_sizes, output_size=10, activation_func='ReLU', dropout=0):\n",
    "    model = Sequential()\n",
    "    last_size = input_size\n",
    "    for hidden_size in hidden_sizes:\n",
    "        model.add(FullyConnected(last_size, hidden_size))\n",
    "        model.add(activation[activation_func]())\n",
    "        last_size = hidden_size\n",
    "    model.add(FullyConnected(last_size, output_size))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -- training options\n",
    "trainopt = {\n",
    "    'eta': .1,   # initial learning rate\n",
    "    'maxiter': 20000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 500,  # display batch loss every display_iter updates\n",
    "    'batch_size': 100,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25  #\n",
    "}\n",
    "\n",
    "NFEATURES = Xlarge.shape[1]\n",
    "trained_models = dict()\n",
    "\n",
    "# set the (initial?) set of lambda values to explore\n",
    "lambdas = np.array([0, 0.001, 0.01, 0.1])\n",
    "hidden_sizes = np.array([10])\n",
    "    \n",
    "for lambda_ in lambdas:\n",
    "    for hidden_size_ in hidden_sizes:\n",
    "        trainopt['lambda'] = lambda_\n",
    "        model = build_model(NFEATURES, hidden_size_, 10, dropout=0)\n",
    "        crit = SoftMaxLoss()\n",
    "        # -- model trained on large train set\n",
    "        trained_model, valErr, trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "        trained_models[(lambda_, hidden_size_)] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "        print('train set model: -> lambda= %.4f, train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
    "    \n",
    "best_trained_lambda = 0.\n",
    "best_trained_model = None\n",
    "best_trained_val_err = 100.\n",
    "for (lambda_, hidden_size_), results in trained_models.items():\n",
    "    print('lambda= %.4f, hidden size: %5d, val error: %.2f' %(lambda_, hidden_size_, results['val_err']))\n",
    "    if results['val_err'] < best_trained_val_err:\n",
    "        best_trained_val_err = results['val_err']\n",
    "        best_trained_model = results['model']\n",
    "        best_trained_lambda = lambda_\n",
    "\n",
    "print(\"Best train model val err:\", best_trained_val_err)\n",
    "print(\"Best train model lambda:\", best_trained_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.298 train error: 90.257 val error: 89.900\n",
      "     500 batch loss: 0.383 train error: 8.929 val error: 8.550\n",
      "    1000 batch loss: 0.236 train error: 6.671 val error: 6.950\n",
      "    1500 batch loss: 0.152 train error: 5.129 val error: 6.350\n",
      "    2000 batch loss: 0.058 train error: 3.643 val error: 6.000\n",
      "    2500 batch loss: 0.095 train error: 2.429 val error: 5.300\n",
      "    3000 batch loss: 0.091 train error: 1.643 val error: 4.950\n",
      "    3500 batch loss: 0.054 train error: 1.200 val error: 4.750\n",
      "    4000 batch loss: 0.051 train error: 0.686 val error: 4.600\n",
      "    4500 batch loss: 0.021 train error: 0.429 val error: 4.800\n",
      "train error: 0.43, val error: 4.60\n"
     ]
    }
   ],
   "source": [
    "# custom model architectures\n",
    "custom_model = build_model_custom(NFEATURES, 400, 400, output_size=10, activation_func='ReLU')\n",
    "crit = SoftMaxLoss()\n",
    "trained_model, valErr, trainErr = runTrainVal(Xlarge, Ylarge, custom_model, Xval, Yval, trainopt)\n",
    "print(f'train error: {trainErr:.2f}, val error: {valErr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission-mnist.csv\n"
     ]
    }
   ],
   "source": [
    "kaggleX = MNIST_utils.load_data(data_fn, 'kaggle')\n",
    "kaggleYhat = predict(kaggleX, custom_model).argmax(-1)\n",
    "save_submission('submission-mnist.csv', kaggleYhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission-mnist.csv\n"
     ]
    }
   ],
   "source": [
    "kaggleX = MNIST_utils.load_data(data_fn, 'kaggle')\n",
    "kaggleYhat = predict(kaggleX, best_trained_model).argmax(-1)\n",
    "save_submission('submission-mnist.csv', kaggleYhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
