{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from ps4_utils import load_data,load_experiment\n",
    "from ps4_utils import AbstractGenerativeModel\n",
    "from ps4_utils import save_submission\n",
    "from scipy.special import logsumexp\n",
    "import numpy as np\n",
    "data_fn = \"datasets-ps4.h5\"\n",
    "MAX_OUTER_ITER = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "jwn = np.array([[1,2,3],[4,5,6]])\n",
    "print(np.sum(jwn, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MixtureModel(AbstractGenerativeModel):\n",
    "    def __init__(self, CLASSES, NUM_FEATURES, NUM_MIXTURE_COMPONENTS, MAX_ITER=50, EPS=1e-7):\n",
    "        super().__init__(CLASSES, NUM_FEATURES)\n",
    "        \n",
    "        self.epsilon = EPS              # clamp theta in [eps, 1-eps]\n",
    "        self.max_iter = MAX_ITER        # iterations of EM algo\n",
    "        self.num_mixture_components = NUM_MIXTURE_COMPONENTS\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['pi'] = [np.repeat(1/k, k) for k in self.num_mixture_components] # mixture probabilities\n",
    "        self.params['theta'] = [np.zeros((self.num_features, k)) for k in self.num_mixture_components] # component probabilities\n",
    "        \n",
    "    def pack_params(self, X, class_idx):\n",
    "        pi, theta = self.fit(X[class_idx], class_idx)\n",
    "        self.params['pi'][class_idx] = pi\n",
    "        self.params['theta'][class_idx] = theta\n",
    "        \n",
    "    def classify(self, X):\n",
    "        P = list()\n",
    "        pi = self.params['pi']\n",
    "        theta = self.params['theta']\n",
    "        for c in range(self.num_classes):\n",
    "            _, Pc = self.findP(X, pi[c], theta[c])\n",
    "            P.append(Pc)\n",
    "        return np.vstack(P).T.argmax(-1)\n",
    "\n",
    "    # --- E-step\n",
    "    def updateLatentPosterior(self, X, pi, theta, _):\n",
    "        logits, norm = self.findP(X, pi, theta)\n",
    "        return np.exp(logits - norm[:, np.newaxis]) \n",
    "    \n",
    "    # --- M-step (1)\n",
    "    @staticmethod\n",
    "    def updatePi(gamma):\n",
    "        return np.sum(gamma, axis=0) / len(gamma)\n",
    "    \n",
    "    # -- M-step (2)\n",
    "    @staticmethod\n",
    "    def updateTheta(X, gamma):\n",
    "        return X.T @ gamma / np.sum(gamma, axis=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def findP(X, pi, theta):\n",
    "        res = np.repeat([np.log(pi)], len(X), axis=0) # NxK\n",
    "        \n",
    "        one_prob = np.log(theta)                      # DxK\n",
    "        zero_prob = np.log(1 - theta)                 # DxK\n",
    "        res += X @ one_prob + (1 - X) @ zero_prob     # NxK\n",
    "        \n",
    "        return res, logsumexp(res, axis=1)            # NxK, N\n",
    "    \n",
    "    @staticmethod\n",
    "    def randomAssignment(X, nmm):\n",
    "        theta = np.zeros((nmm, len(X[0])))\n",
    "        \n",
    "        # imperative for now\n",
    "        for x in X:\n",
    "            theta[np.random.randint(0, nmm)] += x\n",
    "            \n",
    "        return (theta / np.sum(theta, axis=1)[:, np.newaxis]).T\n",
    "        \n",
    "    # --- execute EM procedure\n",
    "    def fit(self, X, class_idx):\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        nmm = self.num_mixture_components[class_idx]\n",
    "        pi = self.params['pi'][class_idx]\n",
    "        theta = np.clip(self.randomAssignment(X, nmm), self.epsilon, 1 - self.epsilon)\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            gamma = self.updateLatentPosterior(X, pi, theta, nmm)\n",
    "            pi = self.updatePi(gamma)\n",
    "            theta = np.clip(self.updateTheta(X, gamma), self.epsilon, 1-self.epsilon)\n",
    "            \n",
    "        return pi, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayesModel(AbstractGenerativeModel):\n",
    "    def __init__(self, CLASSES, NUM_FEATURES, EPS=1e-12):\n",
    "        super().__init__(CLASSES, NUM_FEATURES)\n",
    "        \n",
    "        # for numerical stability\n",
    "        self.eps = EPS\n",
    "        \n",
    "        # p[i][j] = p (jth feature = 1 | class i)\n",
    "        self.params = {'p': [np.zeros((NUM_FEATURES))] * self.num_classes} \n",
    "        \n",
    "    def pack_params(self, X, class_idx):\n",
    "        self.params['p'][class_idx] = self.fit(X[class_idx])\n",
    "        \n",
    "    def classify(self, X):\n",
    "        res = np.zeros(len(X))\n",
    "        \n",
    "        # each of these is CxD\n",
    "        one_prob = np.log(np.array(self.params['p']) + self.eps) \n",
    "        zero_prob = np.log(1 + self.eps - np.array(self.params['p']))\n",
    "        \n",
    "        # this is NxC\n",
    "        logits = X @ one_prob.T + (1 - X) @ zero_prob.T\n",
    "        \n",
    "        # take maximum along rows\n",
    "        return np.argmax(logits, axis=1)\n",
    "    \n",
    "    def fit(self, X) -> np.ndarray:\n",
    "        return np.sum(X, axis=0) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTIMENT ANALYSIS -- NAIVE BAYES MODEL:\n",
      "ACCURACY ON VALIDATION: 0.74\n",
      "SENTIMENT ANALYSIS -- MIXTURE MODEL:\n",
      "COMPONENTS: 12 5\n",
      "ACCURACY ON VALIDATION: 0.714\n",
      "COMPONENTS: 6 9\n",
      "ACCURACY ON VALIDATION: 0.68\n",
      "COMPONENTS: 8 6\n",
      "ACCURACY ON VALIDATION: 0.712\n",
      "COMPONENTS: 14 4\n",
      "ACCURACY ON VALIDATION: 0.714\n",
      "COMPONENTS: 4 13\n",
      "ACCURACY ON VALIDATION: 0.696\n",
      "COMPONENTS: 8 6\n",
      "ACCURACY ON VALIDATION: 0.742\n",
      "COMPONENTS: 4 3\n",
      "ACCURACY ON VALIDATION: 0.712\n",
      "COMPONENTS: 4 6\n",
      "ACCURACY ON VALIDATION: 0.746\n",
      "COMPONENTS: 12 9\n",
      "ACCURACY ON VALIDATION: 0.698\n",
      "COMPONENTS: 12 11\n",
      "ACCURACY ON VALIDATION: 0.736\n",
      "COMPONENTS: 9 13\n",
      "ACCURACY ON VALIDATION: 0.728\n",
      "COMPONENTS: 8 9\n",
      "ACCURACY ON VALIDATION: 0.71\n",
      "COMPONENTS: 14 14\n",
      "ACCURACY ON VALIDATION: 0.71\n",
      "COMPONENTS: 7 7\n",
      "ACCURACY ON VALIDATION: 0.718\n",
      "COMPONENTS: 13 3\n",
      "ACCURACY ON VALIDATION: 0.714\n",
      "Saved: mm-sentiment_analysis-submission.csv\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"sentiment_analysis\"\n",
    "# --- SENTIMENT ANALYSIS setup\n",
    "Xtrain, Xval, num_classes, num_features = load_experiment(data_fn, experiment_name)\n",
    "\n",
    "# -- build naive bayes model for sentiment analysis\n",
    "print(\"SENTIMENT ANALYSIS -- NAIVE BAYES MODEL:\")\n",
    "nbm = NaiveBayesModel(num_classes, num_features)\n",
    "nbm.train(Xtrain)\n",
    "print(\"ACCURACY ON VALIDATION: \" + str(nbm.val(Xval)))\n",
    "\n",
    "# -- build mixture model for sentiment analysis\n",
    "print(\"SENTIMENT ANALYSIS -- MIXTURE MODEL:\")\n",
    "for i in range(MAX_OUTER_ITER):\n",
    "    num_mixture_components =  np.random.randint(2,15,num_classes)\n",
    "    print(\"COMPONENTS: \" + \" \".join(str(i) for i in num_mixture_components))\n",
    "    mm = MixtureModel(num_classes, num_features, num_mixture_components)\n",
    "    mm.train(Xtrain)\n",
    "    print(\"ACCURACY ON VALIDATION: \" + str(mm.val(Xval)))\n",
    "\n",
    "# submit to kaggle\n",
    "Xkaggle = load_data(data_fn, experiment_name, \"kaggle\")\n",
    "save_submission(\"mm-{}-submission.csv\".format(experiment_name), mm.classify(Xkaggle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST DIGIT CLASSIFICATION -- NAIVE BAYES MODEL:\n",
      "ACCURACY ON VALIDATION: 0.7355\n",
      "\n",
      "MNIST DIGIT CLASSIFICATION -- MIXTURE MODEL:\n",
      "COMPONENTS: 3 7 14 12 9 11 14 8 8 10\n",
      "ACCURACY ON VALIDATION: 0.7865\n",
      "COMPONENTS: 5 14 6 13 3 4 9 14 2 8\n",
      "ACCURACY ON VALIDATION: 0.779\n",
      "COMPONENTS: 2 7 9 10 5 8 8 9 7 9\n",
      "ACCURACY ON VALIDATION: 0.7855\n",
      "COMPONENTS: 12 4 13 5 14 3 14 14 13 6\n",
      "ACCURACY ON VALIDATION: 0.7615\n",
      "COMPONENTS: 9 4 4 7 2 6 3 8 7 14\n",
      "ACCURACY ON VALIDATION: 0.7755\n",
      "COMPONENTS: 12 13 5 7 14 4 13 2 5 7\n",
      "ACCURACY ON VALIDATION: 0.771\n",
      "COMPONENTS: 14 10 8 2 2 2 14 5 12 11\n",
      "ACCURACY ON VALIDATION: 0.757\n",
      "COMPONENTS: 13 10 9 7 8 13 3 12 12 14\n",
      "ACCURACY ON VALIDATION: 0.772\n",
      "COMPONENTS: 9 12 5 3 9 12 13 10 14 5\n",
      "ACCURACY ON VALIDATION: 0.7705\n",
      "COMPONENTS: 5 3 9 14 2 10 2 11 8 7\n",
      "ACCURACY ON VALIDATION: 0.781\n",
      "COMPONENTS: 2 5 3 7 10 12 2 4 10 14\n",
      "ACCURACY ON VALIDATION: 0.761\n",
      "COMPONENTS: 9 11 9 13 9 5 7 9 9 8\n",
      "ACCURACY ON VALIDATION: 0.7855\n",
      "COMPONENTS: 14 8 14 11 8 13 4 8 6 10\n",
      "ACCURACY ON VALIDATION: 0.775\n",
      "COMPONENTS: 2 8 13 2 3 6 11 11 2 5\n",
      "ACCURACY ON VALIDATION: 0.7655\n",
      "COMPONENTS: 9 4 4 9 12 12 14 9 9 8\n",
      "ACCURACY ON VALIDATION: 0.786\n",
      "Saved: mm-mnist-submission.csv\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"mnist\"\n",
    "# --- MNIST DIGIT CLASSIFICATION setup\n",
    "Xtrain, Xval, num_classes, num_features = load_experiment(data_fn, experiment_name)\n",
    "\n",
    "# -- build naive bayes model for mnist digit classification\n",
    "print(\"MNIST DIGIT CLASSIFICATION -- NAIVE BAYES MODEL:\")\n",
    "nbm = NaiveBayesModel(num_classes, num_features)\n",
    "nbm.train(Xtrain)\n",
    "print(\"ACCURACY ON VALIDATION: \" + str(nbm.val(Xval)))\n",
    "print()\n",
    "\n",
    "# -- build mixture model for mnist digit classification\n",
    "print(\"MNIST DIGIT CLASSIFICATION -- MIXTURE MODEL:\")\n",
    "for i in range(MAX_OUTER_ITER):\n",
    "    num_mixture_components =  np.random.randint(2, 15, num_classes)\n",
    "    print(\"COMPONENTS: \" + \" \".join(str(i) for i in num_mixture_components))\n",
    "    mm = MixtureModel(num_classes, num_features, num_mixture_components)\n",
    "    mm.train(Xtrain)\n",
    "    print(\"ACCURACY ON VALIDATION: \" + str(mm.val(Xval)))\n",
    "    \n",
    "# submit to kaggle\n",
    "Xkaggle = load_data(data_fn, experiment_name, \"kaggle\")\n",
    "save_submission(\"mm-{}-submission.csv\".format(experiment_name), mm.classify(Xkaggle))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
