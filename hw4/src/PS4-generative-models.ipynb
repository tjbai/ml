{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from ps4_utils import load_data,load_experiment\n",
    "from ps4_utils import AbstractGenerativeModel\n",
    "from ps4_utils import save_submission\n",
    "from scipy.special import logsumexp\n",
    "import numpy as np\n",
    "data_fn = \"datasets-ps4.h5\"\n",
    "MAX_OUTER_ITER = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MixtureModel(AbstractGenerativeModel):\n",
    "    def __init__(self, CLASSES, NUM_FEATURES, NUM_MIXTURE_COMPONENTS, MAX_ITER=50, EPS=10**(-7)):\n",
    "        AbstractGenerativeModel.__init__(self, CLASSES, NUM_FEATURES)\n",
    "        self.num_mixture_components = NUM_MIXTURE_COMPONENTS # list of num_mixture_components (length num_classes)\n",
    "        self.max_iter = MAX_ITER # max iterations of EM\n",
    "        self.epsilon = EPS # help with stability, to be used according to hint given at end of pset4.pdf\n",
    "        self.params = { # lists of length CLASSES\n",
    "            'pi': [np.repeat(1/k,k) for k in self.num_mixture_components], # with pi_c for each class\n",
    "            'theta': [np.zeros((self.num_features,k)) for k in self.num_mixture_components], # with theta_c for each class\n",
    "        }\n",
    "    def pack_params(self, X, class_idx):\n",
    "        pi,theta = self.fit(X[class_idx],class_idx) # fit parameters\n",
    "        self.params['pi'][class_idx] = pi # update member variable pi\n",
    "        self.params['theta'][class_idx] = theta #update member variable theta\n",
    "        \n",
    "    # make classification based on which mixture model gives higher probability to generating point xi\n",
    "    def classify(self, X):\n",
    "        P = list()\n",
    "        pi = self.params['pi']\n",
    "        theta = self.params['theta']\n",
    "        for c in range(self.num_classes):\n",
    "            _,Pc = self.findP(X, pi[c], theta[c])\n",
    "            P.append(Pc)\n",
    "        return np.vstack(P).T.argmax(-1) # np.array of class predictions for each data point in X\n",
    "\n",
    "    # --- E-step\n",
    "    def updateLatentPosterior(self, X, pi, theta, num_mixture_components): # update the latent posterior\n",
    "        # YOUR CODE HERE\n",
    "        # --- gamma: responsibilities (probabilities), np.array (matrix)\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c) by NUM_MIXTURE_COMPONENTS[c]\n",
    "        # note: can use output of findP here (with care taken to return gamma containing proper probabilities)\n",
    "        return gamma\n",
    "    \n",
    "    # --- M-step (1)\n",
    "    @staticmethod\n",
    "    def updatePi(gamma): #update the pi component using the posteriors (gammas)\n",
    "        # YOUR CODE HERE\n",
    "        # --- pi_c: class specific pi, np.array (vector)\n",
    "        # ---        shape: NUM_MIXTURE_COMPONENTS[c]\n",
    "        return pi_c\n",
    "    \n",
    "    # -- M-step (2)\n",
    "    @staticmethod\n",
    "    def updateTheta(X, gamma): #update theta component using posteriors (gammas)\n",
    "        # YOUR CODE HERE\n",
    "        # --- theta_c: class specific theta, np.array matrix\n",
    "        # ---        shape: NUM_FEATURES by NUM_MIXTURE_COMPONENTS[c]\n",
    "        return theta_c \n",
    "    \n",
    "    @staticmethod\n",
    "    def findP(X, pi, theta):\n",
    "        # YOUR CODE HERE\n",
    "        # NOTE: you can also use t as a probability, just change \"logsumexp(t,axis=1)\" to \"logsumexp(np.log(t),axis=1)\"\n",
    "        # --- t: logprobabilities of x given each component of mixture\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c) by NUM_MIXTURE_COMPONENTS[c] \n",
    "        # --- logsumexp(t,axis=1): (for convenience) once exponentiated, gives normalization factor over all mixture components\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c)\n",
    "        return t, logsumexp(t, axis=1)\n",
    "        \n",
    "    # --- execute EM procedure\n",
    "    def fit(self, X, class_idx):\n",
    "        max_iter = self.max_iter\n",
    "        eps = self.epsilon\n",
    "        N = X.shape[0]\n",
    "        pi = self.params['pi'][class_idx]\n",
    "        theta = self.params['theta'][class_idx]\n",
    "        num_mixture_components = self.num_mixture_components[class_idx]\n",
    "        # INITIALIZE theta, note theta is currently set to zeros but needs to be officially initialized here\n",
    "        for i in range(max_iter):\n",
    "            # YOUR CODE HERE, E-step: gamma = self.updateLatentPosterior\n",
    "            # YOUR CODE HERE, M-step(1): pi = self.updatePi \n",
    "            # YOUR CODE HERE, M-step(2): theta = self.updateTheta\n",
    "        return pi, theta #pi and theta, given class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayesModel(AbstractGenerativeModel):\n",
    "    def __init__(self, CLASSES, NUM_FEATURES, EPS=10**(-12)):\n",
    "        super().__init__(CLASSES, NUM_FEATURES)\n",
    "        \n",
    "        # for numerical stability\n",
    "        self.eps = EPS\n",
    "        \n",
    "        # p[i][j] = p (jth feature = 1 | class i)\n",
    "        self.params = {'p': [np.zeros((NUM_FEATURES))] * self.num_classes} \n",
    "        \n",
    "    def pack_params(self, X, class_idx):\n",
    "        self.params['p'][class_idx] = self.fit(X[class_idx])\n",
    "        \n",
    "    def classify(self, X):\n",
    "        res = np.zeros(len(X))\n",
    "        \n",
    "        # each of these is CxD\n",
    "        one_prob = np.log(np.array(self.params['p']) + self.eps) \n",
    "        zero_prob = np.log(1 + self.eps - np.array(self.params['p']))\n",
    "        \n",
    "        # this is NxC\n",
    "        logits = X @ one_prob.T + (1 - X) @ zero_prob.T\n",
    "        \n",
    "        # take maximum along rows\n",
    "        return np.argmax(logits, axis=1)\n",
    "    \n",
    "    def fit(self, X) -> np.ndarray:\n",
    "        return np.sum(X, axis=0) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTIMENT ANALYSIS -- NAIVE BAYES MODEL:\n",
      "(827, 4287)\n",
      "(1448, 4287)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb Cell 4\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m nbm \u001b[39m=\u001b[39m NaiveBayesModel(num_classes, num_features)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m nbm\u001b[39m.\u001b[39mtrain(Xtrain)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mACCURACY ON VALIDATION: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(nbm\u001b[39m.\u001b[39;49mval(Xval)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# -- build mixture model for sentiment analysis\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSENTIMENT ANALYSIS -- MIXTURE MODEL:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/ml/hw4/src/ps4_utils.py:46\u001b[0m, in \u001b[0;36mAbstractGenerativeModel.val\u001b[0;34m(self, X, acc, N)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mval\u001b[39m(\u001b[39mself\u001b[39m, X, acc\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, N\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes):\n\u001b[0;32m---> 46\u001b[0m         acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassify(X[c]) \u001b[39m==\u001b[39m c)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint32))\n\u001b[1;32m     47\u001b[0m         N \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m X[c]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m (acc \u001b[39m/\u001b[39m N)\n",
      "\u001b[1;32m/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify\u001b[39m(\u001b[39mself\u001b[39m, X): \u001b[39m# naive bayes classifier\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# --- predictions: predictions for data points in X (where X consists of datapoints from class c), np.array (vector)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# ---        shape: number of data points\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bai/Desktop/ml/hw4/src/PS4-generative-models.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m predictions\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "experiment_name = \"sentiment_analysis\"\n",
    "# --- SENTIMENT ANALYSIS setup\n",
    "Xtrain, Xval, num_classes, num_features = load_experiment(data_fn, experiment_name)\n",
    "\n",
    "# -- build naive bayes model for sentiment analysis\n",
    "print(\"SENTIMENT ANALYSIS -- NAIVE BAYES MODEL:\")\n",
    "nbm = NaiveBayesModel(num_classes, num_features)\n",
    "nbm.train(Xtrain)\n",
    "print(\"ACCURACY ON VALIDATION: \" + str(nbm.val(Xval)))\n",
    "\n",
    "# -- build mixture model for sentiment analysis\n",
    "print(\"SENTIMENT ANALYSIS -- MIXTURE MODEL:\")\n",
    "for i in range(MAX_OUTER_ITER):\n",
    "    num_mixture_components =  np.random.randint(2,15,num_classes)\n",
    "    print(\"COMPONENTS: \" + \" \".join(str(i) for i in num_mixture_components))\n",
    "    mm = MixtureModel(num_classes, num_features, num_mixture_components)\n",
    "    mm.train(Xtrain)\n",
    "    print(\"ACCURACY ON VALIDATION: \" + str(mm.val(Xval)))\n",
    "\n",
    "# submit to kaggle\n",
    "Xkaggle = load_data(data_fn, experiment_name, \"kaggle\")\n",
    "save_submission(\"mm-{}-submission.csv\".format(experiment_name), mm.classify(Xkaggle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST DIGIT CLASSIFICATION -- NAIVE BAYES MODEL:\n",
      "ACCURACY ON VALIDATION: 0.7355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# -- build mixture model for mnist digit classification\\nprint(\"MNIST DIGIT CLASSIFICATION -- MIXTURE MODEL:\")\\nfor i in range(MAX_OUTER_ITER):\\n    num_mixture_components =  np.random.randint(2,15,num_classes)\\n    print(\"COMPONENTS: \" + \" \".join(str(i) for i in num_mixture_components))\\n    mm = MixtureModel(num_classes, num_features, num_mixture_components)\\n    mm.train(Xtrain)\\n    print(\"ACCURACY ON VALIDATION: \" + str(mm.val(Xval)))\\n    \\n# submit to kaggle\\nXkaggle = load_data(data_fn, experiment_name, \"kaggle\")\\nsave_submission(\"mm-{}-submission.csv\".format(experiment_name), mm.classify(Xkaggle))\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = \"mnist\"\n",
    "# --- MNIST DIGIT CLASSIFICATION setup\n",
    "Xtrain, Xval, num_classes, num_features = load_experiment(data_fn, experiment_name)\n",
    "\n",
    "# -- build naive bayes model for mnist digit classification\n",
    "print(\"MNIST DIGIT CLASSIFICATION -- NAIVE BAYES MODEL:\")\n",
    "nbm = NaiveBayesModel(num_classes, num_features)\n",
    "nbm.train(Xtrain)\n",
    "print(\"ACCURACY ON VALIDATION: \" + str(nbm.val(Xval)))\n",
    "\n",
    "# -- build mixture model for mnist digit classification\n",
    "print(\"MNIST DIGIT CLASSIFICATION -- MIXTURE MODEL:\")\n",
    "for i in range(MAX_OUTER_ITER):\n",
    "    num_mixture_components =  np.random.randint(2,15,num_classes)\n",
    "    print(\"COMPONENTS: \" + \" \".join(str(i) for i in num_mixture_components))\n",
    "    mm = MixtureModel(num_classes, num_features, num_mixture_components)\n",
    "    mm.train(Xtrain)\n",
    "    print(\"ACCURACY ON VALIDATION: \" + str(mm.val(Xval)))\n",
    "    \n",
    "# submit to kaggle\n",
    "Xkaggle = load_data(data_fn, experiment_name, \"kaggle\")\n",
    "save_submission(\"mm-{}-submission.csv\".format(experiment_name), mm.classify(Xkaggle))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
